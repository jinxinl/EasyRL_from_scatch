# Ch1 强化学习基础



[TOC]

## 强化学习概述

- RL目标：在复杂、不确定的环境中最大化奖励，也就是在与环境的多轮互动中，学习到最大化奖励的策略

- RL挑战：

  - 奖励不是及时反馈的，做出当前行动时并不知道会对最终结果产生什么影响—— **延迟奖励** 
  - 智能体的动作会影响它后续得到的数据，，因此需要保证智能体的动作能够稳步提升
  - 近期奖励和长期奖励的权衡，如何获得更多的长期奖励（短期奖励更多 $≠$ 长期奖励更多）

- RL基本流程：RL中有两个关键要素，智能体和环境，智能体从环境中得到当前状态 $s_t$ ，并根据 $s_t$ 做出行动 $a_t$ ，也叫作决策，$a_t$ 会在环境中执行，接着环境会输出新的状态 $s_{t+1}$ ，以及 $a_t$ 所带来的奖励 $r_t$，而这会反馈给智能体，进行下一轮行动，直到到达最终状态。这一整个过程叫做回合 $episode$ 或试验 $trial$ ，智能体在这一回合中的动作序列叫做轨迹/策略 $trajectory$ 

- RL v.s. supervised learning

  | 方面       | RL                                                           | supervised learning                                          |
  | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 数据       | 序列数据，时间关联性强                                       | 假设数据都是独立同分布，消除数据之间的关联性，否则会导致训练不稳定 |
  | 标签       | 延迟奖励，要等到结束时回看才能判断每一步动作的奖励           | 每个数据都有对应的标签，告诉模型输出结果是否正确             |
  | 探索与利用 | 模型需要权衡探索 `exploration` 和利用 `exploitation` ，前者是探索从未尝试过的新动作，能够得到多少奖励未知，后者是在已经尝试的动作中选择奖励最大的作为下一步行动 | 无                                                           |



## 序列决策

- 三要素
  - 智能体和环境：环境给出反馈，智能体从反馈中学习到能够最大化奖励的策略
  - 奖励：环境给出的标量的反馈信号，不同环境、不同任务，给出的反馈也不同
  - 序列决策：
    - 状态：对世界的完整描述
    - 观测：对状态的部分描述
    - 完全可观测：智能体的状态和环境状态等价。这时，RL通常被建模成马尔可夫决策过程 `Markov decision process, MDP` 
    - 部分可观测：智能体得到的状态并不能包含环境运作的状态，即只能看到部分的观测，这时，RL会被建模成部分可观测马尔可夫决策过程 `partially observable Markov decision process` 
    - 用七元组描述：$(S,A,T,R,\Omega,O,\gamma)$ ，$S$ 是状态空间，是隐变量，$A$ 是动作空间，$T(s'|s,a)$ 是状态转移概率，$R$ 是奖励函数，$\Omega(o|s,a)$ 是观测概率，$O$ 是观测空间，$\gamma$ 是折扣系数

## 动作空间 $A$  

有效动作的集合，有离散有连续

##  RL中Agent的组成成分和类型

### 组成成分

- 策略 `policy` ：智能体根据策略选择下一个行动

  - 随机性策略 `stochastic policy` ：即 $\pi$ 函数，$\pi(a|s)=p(a_t=a|s_t=s)$ ，输入一个状态 $s_t$ ，输出概率 $p(a_t=a|s_t=s_t)$ 。通过 $\pi$ 函数可以得到智能体下一步所有动作的概率分布，对这个概率分布进行采样，得到下一步动作。
  - 确定性策略：智能体直接采取最有可能的动作，即 *贪心策略* ，$a^*=\arg\max_{a}\pi(a|s)$ 
  - 一般而言，强化学习一般使用随机性策略，因为（1）随机性策略引入了随机性，能够更好地对动作空间进行探索（2）随机性策略输出的动作具有多样性，在智能体对抗时，若是对于同样状态总是采取相同动作，那么策略很容易被对手预测，不利于获胜

- 价值函数 `value function` ：对当前的状态进行评估，判断当前状态对于后续奖励的影响，即未来累计奖励的预测，选择价值函数值大的状态。

  - 折扣因子 `discount factor`：长期奖励需要“打折”，我们希望在尽可能短的时间里获得奖励越多越好。

  - 价值函数表达式，期望的下标是策略 $\pi$ ，
    $$
    V_{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infin}\gamma^kr_{t+k+1}|s_t=s],\space \text{对所有}s\in S
    $$

  - $Q$ 函数，未来获得的奖励的期望取决于当前状态和采取的行动，表达式如下：
    $$
    Q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|s_t=s,a_t=a]=\mathbb{E}_{\pi}[\sum_{k=0}^{\infin}\gamma^kr_{t+k+1}|s_t=s,a_t=a]
    $$
    价值函数表示的是未来奖励的策略函数 $\pi$ 的长期价值，而 $Q$ 函数则是评估动作 $a$ 的长期价值。前者依赖于 $\pi$ 函数，需要依靠它来选择下一步行动，评估的是策略函数的长期价值，后者独立于 $\pi$ 函数评价动作 $a$ 的价值

- 模型 `model` ：智能体对环境状态的理解，决定了下一步状态（由当前状态 $s_t$ 和行动 $a_t$ 共同组成），决定了环境中世界的运行方式。它由 *状态转移概率* 和 *奖励函数* 两部分组成

  - 状态转移概率：$p^a_{ss'}=p(s_{t+1}=s'|s_t=s,a_t=a)$ 
  - 奖励函数：$R(s,a)=\mathbb{E}[r_{t+1}|s_t=s,a_t=a]$ 


当有了策略、价值函数、模型三个部分后，就形成了一个马尔可夫决策过程 `Markov decision process` 

### 强化学习类型

对一个状态转移矩阵已知的马尔可夫决策过程，可以使用动态规划算法进行求解。决策方式是智能体在给定状态下从动作集合中选择动作的依据，是静态的方法。

- 基于策略的强化学习 `policy-based RL` ：使用学习好的策略函数 $\pi$ 来指导下一步行动
  - 智能体会制定一套策略，从策略中选择一个动作，针对策略函数进行优化，从而让策略能够最大化累积奖励
  - 动作集合庞大、连续，环境连续的场景也适用
  - 常见算法：策略梯度 `Policy Gradient, PG` 
- 基于价值的强化学习 `value-based RL` ：使用价值函数作为指导，朝着价值越大的方向行动
  - 不制定策略，而是维护一个价值函数/价值表格，每次从中选择价值最大的动作作为下一步动作。
  - 动作集合不能太大，不连续、离散的环境
  - 常见算法：$Q$ 学习 `Q-learning` 、$Sarsa$ 

### 强化学习智能体类型

#### 按照决策方式分类

- 基于策略的智能体 `policy-based agent` ：显式学习策略函数，没有学习价值函数
- 基于价值的智能体 `value-based agent` ：显式学习价值函数，隐式学习策略函数，策略是从学到的价值函数中抽取而来的
- 演员-评论员智能体 `actor-critic agent` ：基于价值的智能体与基于策略的只智能体相结合，既学习策略函数又学习价值函数，通过二者交互得到最佳动作

#### 按照智能体有没有学习环境分类

- 有模型智能体 `model-based agent` ：学习状态转移来采取下一步动作

  使用马尔可夫决策过程表示强化学习，可以用四元组表示 $<S,A,P,R>$ ，分别代表状态空间、有效动作集合、状态转移概率矩阵、奖励函数。若是所有元素均已知，那么可以对真实环境进行建模，在构建的虚拟空间中与环境交互。因为只要知道 $P(s_{t+1}=s'|s_t=s,a_t=t)$ 和 $R(r_{t+1}|s_t=s,a_t=a)$ ，就可以知道当前状态下采取某个行动能够获得的奖励和转移到下一状态的概率，这样智能体无需在真实世界中行动，只需要在虚拟环境中学习和规划策略即可

- 免模型智能体 `model-free agent` ：没有去估计状态的转移，也没有得到环境中的具体转移变量，即没有学习状态转移的模型。通过学习价值函数和策略函数选择动作

  现实世界中大多数情况下，转移概率和奖励函数很难估计，模型无法对真实世界建模，只能在环境中按照一定策略行动，得到奖励和状态迁移，然后根据反馈信息动态更新策略，反复迭代直至 $\pi$ 最优。

`model-based agent` 和 `model-free agent` 最大的区别在于有没有对真实世界进行建模（前者有后者无，前者与后者相比只多了这一个步骤）。实际使用时，可以考率模型是否能够仅根据当前状态预测出下一状态和奖励，若是可以，则使用 `model-based agent` ，若不可，使用 `model-free agent` 。

`model-free agent` 是数据驱动型方法，需要采样大量数据来估计状态、奖励函数和动作，而 `model-based agent` 在一定程度上可以缓解数据匮乏问题，因为智能体与虚拟世界交互，在虚拟世界中训练，可以预测下一步状态和奖励，不需要大量数据。

`model-free agent` 的泛化性强于 `model-based agent` ，因为前者是在真实世界中根据反馈不断调整策略，能够更好地适应真实世界的情况，而后者是在与虚拟世界交互的过程中训练的，（1）虚拟世界和真实世界会有偏差，限制了RL算法的泛化性。

但是，`model-based agent` 比 `model-free agent` 更有想象力，他会预测后续会发生的事情，并选择最优策略，而 `model-free agent` 只能根据真实世界的反馈一步一步更新策略。

现代大部分智能体都选择 `model-free agent` ，因为（1）它有大量的简单且丰富的开源资料（2）目前大部分环境是离散、可观察的，这种相对简单、确定的问题不需要评估状态转移函数和奖励函数，使用大量数据训练即可取得较好效果

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250409145227928.png" alt="image-20250409145227928" style="zoom:33%;" />

## 规划与学习

学习 `learning` 与规划 `planning` 是序列决策的基本问题。

- 学习：通过与环境的交互，学习策略函数/价值函数（`model-free agent`），或是环境模型（`model-based agent` ，此时是在虚拟环境中，这样可以加速学习）

  解决模型未知的问题，数据 $\rightarrow$ 策略/模型

- 规划：在已知环境模型的基础上，无需与环境交互（当然也可以和模拟环境相结合来生成虚拟交互数据，如MCTS），只需要知道当前状态，就可以自己开始规划，寻找最优策略（使用动态规划/搜索）

  解决模型已知的问题，模型 $\rightarrow$ 策略

## 探索与利用

- 探索 `exploration` ：通过试错发现新动作对应的奖励，看能不能带来好的奖励
- 利用 `exploitation` ：直接采取当前奖励最大的动作

权衡：如何牺牲短期奖励去进行探索，金额学习到更好的策略

仍然与监督学习不同，监督学习能够在每一次得到结果之后立即判断预测结果与真实值之间的差距，但是强化学习的奖励需要多步之后才能观察到，并智能体需要通过试错来发现各个动作产生的结果，而监督学习可以直接通过标签学习到，相当于被告知了下一步动作的标准答案。

- 单步强化学习任务

  - 最大化单步奖励：一方面是需要尽可能多地知道每个动作对应的奖励，另一方面时执行当前奖励最大的动作。若是每个动作的奖励值是一个最大值，那么所有动作探索一遍就能找到最大奖励的动作，但是实际上每个动作对应的奖励是一个概率分布，仅通过一次尝试没办法得到较为确切的期望奖励。

  - 对应的理论模型：$K-$ 臂赌博机 `K-armed bandit` 。

    - 模型描述：$K-$ 臂赌博机有 $K$ 个摇臂，有一个赌徒，他每次投入一个硬币就可以选择按下其中一个摇臂，每个摇臂都有一定概率吐出硬币。赌徒的目标是最大化奖励，也就是最大化硬币数量

    - 目标：最大化赌徒的硬币数量

    - 仅探索法 `exploration-only` ：仅为获知每个摇臂的期望奖励，那么将所有尝试机会平均分配给每个摇臂（轮流按下），最后以每个摇臂吐出的硬币数量作为奖励期望的近似估计

      仅利用法 `exploitation-only` ：仅为执行奖励最大的动作，只按下目前奖励期望最大的摇臂（若不止一个则在其中随机选一个）

    - 强化学习面临的 **探索-利用窘境 `exploration-exploitation dilemma`**：赌徒的硬币是有限的，也就是尝试机会是有限的，加强了探索（估计摇臂的优劣）必定会削弱了利用（选择目前最优摇臂），反之亦然，因此为了最大化累积奖励，需要权衡 *探索与利用* 

      <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250409153959897.png" alt="image-20250409153959897" style="zoom:50%;" />

## 常用RL包

- Gym
