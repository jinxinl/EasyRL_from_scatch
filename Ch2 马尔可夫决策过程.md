# Ch2 马尔可夫决策过程

[TOC]

马尔可夫决策过程是强化学习的框架，它是指 $S\rightarrow A\rightarrow R\rightarrow S'$ 

<img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250409185846779.png" alt="image-20250409185846779" style="zoom:50%;" />

## 马尔可夫过程 

### 马尔可夫性质 `Markov property`

- 定义：一个随机过程在给定现在状态及所有过去状态的情况下，未来状态的条件概率分布仅与当前状态有关

- 公式：对于由一组随机变量 $X_i$ 构成的随机过程 $X_1,...,X_{t+1}$ ，随机变量的所有取值构成 **状态空间** ，若是该随机过具有马尔可夫性质，那么满足： 
  $$
  P(X_{t+1}=x_{t+1}|X_{0:t}=x_{0:t})=P(X_{t+1}=x_{t+1}|X_t=x_t)
  $$
  即 $X_{t+1}$ 对于过去的条件概率分布函数只是关于 $X_t$ 的函数，未来的转移与过去是独立的，它只取决于现在。

- 马尔可夫性质是所有马尔可夫过程的基础

### 马尔科夫链

- 马尔可夫过程 `Markov process` ：

  - 定义：一组具有马尔可夫性质的随机变量序列 $s_1,s_2,...,s_t$ 称为马尔科夫链。下一时刻的状态 $s_{t+1}$ 只取决于现在状态 $s_t$ 。假设状态的历史为 $h_t={s_1,...,s_t}$ ，那么马尔可夫过程满足：
    $$
    P(s_{t+1}|h_t)=P(s_{t+1}|s_t)
    $$

- 马尔科夫链 `Markov chain` ：

  - 离散时间的马尔可夫过程叫做马尔科夫链，其状态是有限的

  - 马尔科夫链是最简单的马尔可夫过程

  - 使用状态转移矩阵 $P$ 来描述状态转移
    $$
    P=\left(
    \begin{array}{cccc}
    p(s_1|s_1) & p(s_2|s_1) & \cdots & p(s_N|s_1) \\
    p(s_1|s_2) & p(s_2|s_2) & \cdots & p(s_N|s_2) \\
    \vdots & \vdots & \ddots & \vdots \\
    p(s_1|s_N) & p(s_2|s_N) & \cdots & p(s_N|s_N)
    \end{array}
    \right)
    $$
    $P$ 类似于条件概率矩阵，能够告诉智能体在当前状态下，到达别的所有状态的转移概率

### 马尔可夫奖励过程

- 概念

  - 折扣因子 $\gamma$ ：未来的奖励需要打折扣，因为（1）有些马尔可夫过程带环，可能会造成无穷的奖励（2）我们无法建立一个完美的模型，模型对未来的评估不一定准确，我们也不信任模型，因此对于未来的奖励需要乘上折扣，我们希望尽早拿到奖励，而不是在未来某个点拿到（3）如果奖励有实际价值，那么我们希望现在拿到，而不是后面再拿到（越早越好）（4）我们更想拿到即时奖励。

    - $\gamma$ 是超参数，可调整
    - $\gamma=0$ ，不考虑未来奖励，只考虑即时奖励
    - $\gamma=1$ ，不打折，对未来奖励一视同仁

  - 范围 `horizon` ：一个回合的长度

  - 回报 `return` ：奖励的逐步累积，假设 $t$ 时刻后的奖励序列为 $r_{t+1},r_{t+2},...,r_{T}$ ，那么回报 $G_t$ 为
    $$
    G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2 r_{t+3}+...+\gamma^{T-t-1} r_{T}
    $$
    
  - 状态价值函数：状态 $s_t$ 的状态价值函数 $V^{t}(s)$ 就是折扣回报 $G_t$ 的期望
    $$
    V^t(s)=\mathbb{E}(G_t|s_t=s)=\mathbb{E}(r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+...+\gamma^{T-t-1}r_{T}|s_t=s)
    $$
  
- 如何计算状态的价值函数

  - 从这个状态开始生成许多条轨迹，计算每条轨迹的回报，回报的均值作为该状态的价值
  - 蒙特卡洛方法 `Monte Carlo, MC` 
  - 贝尔曼方程 `Bellman equation` 

## 贝尔曼方程

- 公式
  $$
  V(s)=R(s)+\gamma\sum_{s'\in S}p(s' \mid s)V(s')
  $$

  - $R(s)$ ：即时奖励
  - $\gamma$ ：折扣因子
  - $V(s')$ ：未来某一个状态的价值
  - $p(s' \mid s)$ ：从状态 $s$ 到 $s'$ 的转移概率
  - $\gamma\sum_{s'\in S}p(s' \mid s)V(s')$ ：未来奖励的折扣总和

- 定义了当前状态与未来状态之间的迭代关系——当前状态的价值函数可以通过未来状态的价值函数计算。这也叫做“动态规划方程”

- 矩阵形式
  $$
  \left(
  \begin{array}{cccc}
  V(s_1) \\
  V(s_2) \\
  \vdots \\
  V(s_N)
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cccc}
  R(s_1) \\
  R(s_2) \\
  \vdots \\
  R(s_N)
  \end{array}
  \right)
  +\gamma
  \left(
  \begin{array}{cccc}
  p(s_1|s_1) & p(s_2|s_1) & \cdots & p(s_N|s_1) \\
  p(s_1|s_2) & p(s_2|s_2) & \cdots & p(s_N|s_2) \\
  \vdots & \vdots & \ddots & \vdots \\
  p(s_1|s_N) & p(s_2|s_N) & \cdots & p(s_N|s_N)
  \end{array}
  \right)
  \left(
  \begin{array}{cccc}
  V(s_1) \\
  V(s_2) \\
  \vdots \\
  V(s_N)
  \end{array}
  \right)
  $$
  当前状态向量是 $[V(s_1),...,V(s_N)]^T$ ，向量 $V$ 乘上转移概率矩阵中的某一行，再乘上折扣因子 $\gamma$ ，加上即时奖励 $R$ ，即可得到每个状态对应的当前价值

  - 求解：$V=R+\gamma PV$

    ​	$(I-\gamma P)V=R$

    ​	$V=(I-\gamma P)^{-1}R$ 

    $(I-\gamma P)^{-1}R$ 即为 $V$ 的解析解。

    **仅限于很小量的马尔可夫奖励过程** ，因为对大矩阵求逆的计算复杂度很高

- 证明思路，具体见原书 $2.2.2$ ，这里只写思路

  - 先证明：
    $$
    \mathbb{E}(V(s_{t+1})|s)=\mathbb{E}(\mathbb{E}(G_{t+1}|s_{t+1})|s)=\mathbb{E}(G_{t+1}|s)
    $$

    1. 重写回报的期望，简写为 

    $$
    \mathbb{E}(g'|s')=\sum_{g'}g'p(g'|s')
    $$

    2. 利用条件概率公式和期望的定义，证明

    $$
    \mathbb{E}[\mathbb{E}[G_{t+1}|s_{t+1}]|s_t]=\mathbb{E}[\mathbb{E}[g'|s']|s]=\mathbb{E}[g'|s]=\mathbb{E}[G_{t+1}|s_t]
    $$

    

  - 再证明贝尔曼方程
    $$
    V(s)=\mathbb{E}(G_{t}|s_t=s)=R(s)+\gamma\mathbb{E}(G_{t+1}|s_t=s)=R(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')
    $$
    
  - 【补充】全期望公式/叠期望公式 `law of iterated expectattions, LIE` ：
    $$
    E(X)=\sum_i E(X|A_i)P(A_i)
    $$

###  计算马尔可夫奖励过程的迭代算法

- 动态规划，即上述贝尔曼方程：自举 `bootstrapping` 的方法不断更新迭代贝尔曼方程，当前后两次更新的结果差距不大时，停止迭代

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250409214350254.png" alt="image-20250409214350254" style="zoom:60%;" />

- 蒙特卡洛

  - 随机产生从当前状态开始的 $N$ 条轨迹，计算这些轨迹的未来奖励的折扣总和 $G_t^i$
  - 计算 $\{G_t^i\}$ 的均值，作为该状态的价值

  <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250409213104333.png" alt="image-20250409213104333" style="zoom:50%;" />

- 时序差分学习 `temporal-difference learning, TD learning` ，它是 `MC` 与 `dp` 的结合

- 区别：
  - 自举：用当前估计值来更新自身的估计，不用等到回合结束
  - 动态规划：提前知道未来走向
  - 蒙特卡洛：需要得到回合结束，对价值进行评估



## 马尔可夫决策过程

### 马尔可夫决策过程

- 马尔可夫决策过程 `Markov decision process, MDP` ：状态转移依赖于当前状态与执行动作，
  $$
  P(s_{t+1} \mid h_t,a_t)=P(s_{t+1} \mid s_t,a_t)
  $$
  动作 $a_t$ 是由智能体选择并执行的。

  - 区别：与马尔可夫过程/马尔可夫奖励过程相比，最大的不同在于 *决策* ，决策代表动作 $a$ 。

    <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250410114535425.png" alt="image-20250410114535425" style="zoom:50%;" />

    - 马尔可夫过程：“随波逐流”，没有策略，没有奖励，只有 $S$ 与 $P$ ，比如天气预报的下一个状态仅依赖于当前天气

    - 马尔可夫奖励过程：“随波逐流”，但是有奖励机制，每到一个地方，会有标签（即时奖励）。虽然可以计算长期奖励，但没有主动决策，

    - 马尔可夫决策过程：智能体选择 $a_t$ 。智能体就像是划桨的船夫，控制策略前进方向，学习如何划桨，划多大力，但同时仍然受到 $P$ 的影响（水流、风向），目标是最大化长期奖励。

  -  `MDP` $\rightarrow$ 马尔可夫奖励过程：
    $$
    P(s'|s)=\sum_{a\in A}\pi(a|s)P(s'|s,a)
    $$
    对于奖励函数 $R(s,a)$ ，也可以把动作去掉，得到类似马尔可夫奖励过程的奖励函数
    $$
    r_{\pi}(s)=\sum_{a\in A}\pi(a|s)R(s,a)
    $$
    
  - 价值函数
    $$
    V_{\pi}(s)=\mathbb{E}[G_t|s_t=s]
    $$
    期望基于策略，当策略确定后，可以对策略进行采样得到期望，计算价值函数 $V_{\pi}(s)$ 的值 
  
  - $Q$ 函数：动作价值函数 `action-value function` ，其定义是在某个状态采取某个动作，可能得到的长期回报的期望。
    $$
    Q_{\pi}(s,a)=\mathbb{E}[G_t|s_t=s,a_t=a]
    $$
    期望也是基于策略。
  
    - 对 $Q$ 函数中的动作进行求和，即可得到价值函数 $V_{\pi}(s)$ 
  
    $$
    V_{\pi}(s)=\sum_{a\in A}\pi(a|s)Q(s,a)
    $$
  
    
  
    - $Q$ 函数的贝尔曼方程
      $$
      Q_{\pi}(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V(s')
      $$
      【价值函数的贝尔曼方程：$V_{\pi}(s)=R(s)+\gamma\sum_{s'\in S}p(s'|s)V(s')$ 】

### 贝尔曼期望方程

贝尔曼期望方程定义了1当前状态与未来状态的关联

- 价值函数：
  $$
  V_{\pi}(s)=\mathbb{E}_{\pi}[r_{t+1}+\gamma V_{\pi}(s_{t+1})|s_t=s]
  $$
  
- $Q$ 函数：
  $$
  Q_{\pi}(s,a)=\mathbb{E}_{\pi}[r_{t+1}+\gamma Q_{\pi}(s_{t+1},a_{t+1})|s_t=s,a_t=a]
  $$

- 改写：

  - 中间桥梁：

    - $$
      V_{\pi}(s)=\sum_{a\in A}\pi(a|s)Q_{\pi}(s,a)
      $$

    - $$
      Q_{\pi}(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V(s')
      $$

  - 利用中间桥梁的两个式子，把 $V$ 和 $Q$ 互相代入得到

    - $Q$ 代入 $V$ ，得到当前状态的价值函数与未来状态的价值函数之间的关联
      $$
      V_{\pi}(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V(s')]
      $$

    - $V$ 代入 $Q$ ，得到当前状态的 $Q$ 函数与未来状态的 $Q$ 函数之间的关联
      $$
      Q_{\pi}(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi(a'|s')Q_{\pi}(s',a')
      $$

    上面两式是贝尔曼期望方程的另一种形式（1）期望形式（2）即时奖励+长期奖励的折扣总和

### 备份图

- 备份 `backup` 
  - 对于某一个状态，当前状态的价值和未来状态的价值是线性关系，类似 `bootsrapping` 之间的迭代关系

- 备份图 `backup diagram` 

  白色圆圈是状态，黑色圆圈是动作。

  备份图展示的关系是更新与备份操作的基础，将后续状态转移回当前状态。转移过程实际上是加权和。状态 $s$ 对状态空间进行加权和，动作 $a$ 对策略函数 $\pi(a|s)$ 进行加权和。

  - $V_{\pi}$ 备份图

    <img src="C:\Users\jinxingling\AppData\Roaming\Typora\typora-user-images\image-20250410133923877.png" alt="image-20250410133923877" style="zoom:33%;" />
    $$
    V_{\pi}(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V(s')]
    $$

    - 对叶结点状态 $s'$ 加权求和：$\sum_{s'\in S}p(s'|s,a)V(s')$ 
    - 对第二层动作 $a$ 加权求和：$\sum_{a\in A}\pi(a|s)Q_{\pi}(s,a)$  

    <img src="C:\Users\jinxingling\AppData\Roaming\Typora\typora-user-images\image-20250410134657878.png" alt="image-20250410134657878" style="zoom:60%;" />

  - $Q_{\pi}$ 备份图

    <img src="C:\Users\jinxingling\AppData\Roaming\Typora\typora-user-images\image-20250410135033849.png" alt="image-20250410135033849" style="zoom:40%;" />
    $$
    Q_{\pi}(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)\sum_{a'\in A}\pi(a'|s')Q(s',a')
    $$

    - 对叶节点动作加和：$\sum_{a'\in A}\pi(a'|s')Q(s',a')$ 

    - 对第二层状态求和：$\sum_{s'\in S}p(s'|s,a)V(s')$

      <img src="C:\Users\jinxingling\AppData\Roaming\Typora\typora-user-images\image-20250410135536661.png" alt="image-20250410135536661" style="zoom:73%;" />

  - 同步备份 `synchronous backup` ：每一次迭代完全更新所有的状态，对资源需求很大
  - 异步备份 `asynchronous backup` ：通过某种方式，每一次迭代不需要更新所有内容，因为实际上也不需要更新这么多状态

### 预测与控制

预测 `predict` 与控制 `control` 是马尔可夫决策过程的核心问题。

- 预测
  - 输入是马尔可夫决策过程 $<S,A,P,R,\gamma>$ 和策略 $\pi$ ，输出是最佳价值函数 $V^*$ 
  - 使用动态规划解决

- 控制
  - 输入是马尔可夫决策过程 $<S,A,P,R,\gamma>$ ，输出是最佳价值函数 $V^*$ 和最佳策略 $\pi^*$
  - 使用动态规划解决

二者的区别在于，预测是给定一个策略，来确定该策略的价值函数 ；控制是在

没有策略的条件下，去寻找最佳价值函数以及对应的决策方案（最佳策略）。

预测和控制是递进的关系，先解决预测问题，再解决控制问题。

【动态规划：前提是问题需要满足（1）最优子结构（2）重叠子问题

   马尔可夫决策过程满足动态规划的要求，递归方程就是贝尔曼方程。但是动态规划是应用于马尔可夫决策过程的规划问题而不是学习问题，需要状态转移和奖励已知（即环境已知）】

### 策略评估——预测

- 含义：已知马尔可夫决策过程的策略函数 $\pi$ ，计算该策略函数的价值函数 $V_{\pi}(s)$ 的过程就是策略评估

- 这是预测问题，因为已知马尔可夫j决策过程和策略函数

- 求解：使用贝尔曼方程计算每个状态的价值
  $$
  V_{\pi}^{k}(s)=r(s,\pi(s))+\gamma\sum_{s'\in S}p(s'|s,\pi(s))V_{\pi}^{k-1}(s')
  $$
  $k$ 是迭代次数，随着迭代次数的增多，最终 $V_{\pi}$ 会收敛，收敛后的状态函数 $V_{\pi}(s)$ 就是每个状态的价值。

### 马尔可夫决策过程的策略评估

【预测问题】

把贝尔曼期望备份变成迭代的过程，反复迭代直至收敛，这个迭代过程可以看成同步备份的过程。

- 贝尔曼期望备份 $\rightarrow$ 动态规划的迭代
  $$
  V^{t+1}(s)=\sum_{a\in A}\pi(a|s)[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V^t(s')]
  $$
  又因为预测问题中已经给定了策略 $\pi$ ，所以可以去掉动作，简写成
  $$
  V^{t+1}(s)=r_{\pi}(s)+\gamma P_{\pi}(s'|s)V^t(s')
  $$
  
  这样得到的式子只与状态有关，在每次迭代中都可以的大片每个状态的价值
  

### 马尔可夫决策过程控制

- 控制：只知道马尔科夫决策，需要找到最佳策略，从而得到最佳价值函数

- 最佳价值函数：寻找一种策略 $\pi$ ，使当前状态的价值最大。$V^*$ 是指到达每一个状态，它的值最大化的情况。此时的策略 $\pi$ 也叫做最佳策略 $\pi^*$ 。
  $$
  V^*_{\pi}(s)=\max_{\pi}V_{\pi}(s)
  $$

- 最佳策略：使每个状态的价值函数取最大值
  $$
  \pi^*=\arg\max_{\pi}V^*_{\pi}(s)
  $$

若是能够得到状态的最佳价值函数，那么认为这个马尔可夫决策过程的环境可解。在这种情况下，最佳价值函数是一致/唯一的，环境可达到的上限值是一致的。可能有多个最佳策略，多个最佳策略可以取得相同的最佳价值函数值。

- 从 $Q^*$ 得到 $\pi^*(a|s)$ ：
  $$
  \pi^*(a|s)=\begin{cases}
  1, \space a=\arg\max_{a\in A}Q^*(s,a) \\
  0, \space otherwise
  \end{cases}
  $$
  因为 $Q$ 函数的变量是当前状态 $s$ 和行动 $a$ ，所以当 $Q$ 函数收敛之后，若是在状态 $s$ 下采取某个动作能够使 $Q$ 函数值最大化，那么这个动作就是最佳的动作。

  所以优化出 $Q^*$ 函数，直接在 $Q$ 函数中取一个让 $Q$ 函数值最大的动作序列，就能够得到最佳策略 $\pi^*$

### 策略搜索

寻找最佳策略的过程就是马尔可夫决策过程的控制过程。马尔可夫决策过程就是寻找一个最佳策略来得到最大的价值函数值.
$$
\pi^*(s)=\arg\max_{\pi}V_{\pi}(s)
$$
最佳策略一般是确定的、稳定的，但不一定唯一，但是其所对应的价值函数最大值是相同的

- 穷举：仅限于动作空间 $A$ 与状态空间 $S$ 都有限的情况，对每个状态 $s$ 均有 $|A|$ 种动作可以执行，那么一共有 $|A|^{|S|}$ 种可能的策略，把每种策略的价值函数计算出来、进行比较，即可得到最佳策略。效率很低

-  策略迭代：

  - 两步骤：

    <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250410192303622.png" alt="image-20250410192303622" style="zoom:53%;" />

    - 策略评估：给定当前策略函数估计其状态价值函数

    - 策略改进 `policy improvement` ：利用估计的状态价值函数 $V_{\pi}(s)$ 得到 $Q$ 函数，接着对 $Q$ 函数做 *贪心搜索* ，最大化每个状态的相应动作，用得到的动作序列更新原本的策略函数 $\pi$ 。

      - 得到 $V_{\pi}(s)$ 后，使用状态转移矩阵和奖励函数计算 $Q$ 函数
        $$
        Q_{\pi_i}(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{\pi_i}(s')
        $$

      - 再使用 $Q$ 函数抽取改进后的策略
        $$
        \pi_{i+1}(s)=\arg\max_{a}Q_{\pi_i}(s,a)
        $$

    - 可以把 $Q$ 函数中看成 $Q$ 表格 `Q-table` ，横轴是 $s$ ，纵轴是 $a$ 。取使 $Q$ 函数值最大的动作相当于取每列最大值对应的动作。

    <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250410193232962.png" alt="image-20250410193232962" style="zoom:50%;" />

    不断进行 *策略评估-策略改进* 的迭代，直到收敛

    <img src="C:\Users\xinling\AppData\Roaming\Typora\typora-user-images\image-20250410192530402.png" alt="image-20250410192530402" style="zoom:65%;" />

  - 贝尔曼最优方程 `bellman optimality equation` ：

    - 策略改进定理：若是有 $\pi^*=\arg\max_{a}Q_{\pi}(s,a)$ ，那么 $\pi$ 对应的价值函数是单调递增的，即 $V_{\pi'}(s)\ge V_{\pi}(s)$ 。

      - 解释：（1）$\pi$ 的选择是根据贪心策略选择能够使当前状态下 $Q$ 值最大的动作，能够直接提升 $V$ 的即时奖励或未来期望（2）因为 $V_{\pi}$ 单调上升且必有上界（$V_{\pi}$ 是长期奖励的 *折扣* 总和），所以 $V_{\pi}$ 会收敛到最优。

      当迭代结束时，$\pi$ 达到了最佳策略函数 $\pi^*$ ，因为 $\pi$ 选择了使 $Q_{\pi}(s,a)$ 最大的动作，而 $V_{\pi}(s)$ 是基于 $\pi$  计算的，因此 $V_{\pi}(s)=\max_{a}Q_{\pi}(s,a)$ ，即
      $$
      V_{\pi}(s)=\max_{a}[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{\pi}(s')]]
      $$
      这正是 **贝尔曼最优方程** 。

    $$
    Q_{\pi}(s,\pi'(s))=Q_{\pi}(s,a)=\max_{a\in A}Q_{\pi}(s,\pi(s))=V_{\pi}(s)
    $$

    - 解释：

      - 第一个等号指 $\pi'(s)$ 是贪心策略，会选择使 $Q_{\pi}(s,a)$ 最大的动作 $a$ 
      - 第二个等号是改进停止，$\pi'(s)=\pi(s)$ ，当前策略已经贪心
      - 第三个等号是指改进结束时 $V_{\pi}(s)$ 等于 $Q_{\pi}(s)$ 的最大值。

      此时由 $\max_{a}Q_{\pi}(s,\pi(s))$ 可看出 $Q_{\pi}$ 只与当前状态有关，这是使用贪心策略 $\pi$ 的特例， $Q$ 和 $V$ 的关系在改进结束时统一。

      【通俗理解：在一个游戏回合中，策略评估就是看当前操作所获得的分数，策略改进相当于寻找能获得最高分的操作。停止改进时，最高分操作的分数 $\max_{a}Q_{\pi}$ 就是当前关卡的理论最高分 $V_{\pi}$ 】

    - 含义：$V_{\pi}(s)=\max_{a\in A}Q_{\pi}(s,a)$ 表明最佳策略的下一个状态的价值必须等于这个状态下取得最好动作得到的回报的期望。当马尔科夫决策过程满足贝尔曼最优方程时，整个 `MDP` 达到最佳状态

      只有当整个状态收敛，得到最佳价值函数后，贝尔曼最优方程才会满足。满足贝尔曼最优方程后，可以采取最大化操作【**贝尔曼最优方程**】：
      $$
      V^*_{\pi}=\max_{a}Q^*(s,a)
      $$
      同样的， $Q$ 函数之间的转移可以通过将上式代入 $Q$ 函数的贝尔曼方程中得到
      $$
      Q^*(s,a)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s)\max_{a}Q^*(s,a)
      $$
      同理，把 $Q$ 函数的贝尔曼方程代入上式可得价值函数之间的转移【**Q函数的贝尔曼最优方程**】
      $$
      V^*(s)=\max_a[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V^*(s')]
      $$

- 价值迭代：

  - 最优性原理定理 `principle of optimal theorom` ：一个策略 $\pi(a|s)$ 的状态 $s$ 达到了最优价值当且仅当 $s$ 能够到达得所有 $s'$ 都到达了最优价值。即若是有 $V_{\pi}(s)=V^*(s)$ ，当且仅当所有 $s'$ 都满足 $V_{\pi}(s')=V^*(s')$

  - 确认性价值迭代 `deterministic value iteration`

    - 若是已知所有子问题 $V^*(s')$ 的最优解，那么就可以使用 *贝尔曼最优方程* 作为迭代规则，更新 $V_{\pi}(s)$ ，得到最优 $V^*(s)$ 。这是价值迭代的过程，迭代规则（贝尔曼最优方程）如下
      $$
      V(s)\leftarrow \max_{a\in A}[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V(s')]
      $$
      只有当整个 `MDP` 都达到最佳状态时，才满足上式。但是可以把上式转化为一个备份的等式，备份的等式也就是迭代的等式，不停迭代贝尔曼最优方程，价值函数就能够不断趋近最佳价值函数。这也是 **价值迭代的精髓** 。

    - 含义：使用贝尔曼最优方程迭代，迭代多次后价值函数就会收敛到最佳 

  - 算法流程

    - 初始化所有状态的价值函数

    - 假设迭代 $K$ 次（$K$ 是让 $V(s)$ 收所需的次数），每次迭代中进行以下步骤：

      - 对所有状态 $s$ 
        $$
        Q_{k+1}(s)=R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{k}(s)
        $$

        $$
        V_{k+1}(s)=\max_{a}Q_{k+1}(s,a)
        $$

    - 迭代后提取最佳策略
      $$
      \pi^*(s)=\arg\max_{a}[R(s,a)+\gamma\sum_{s'\in S}p(s'|s,a)V_{K+1}(s')]
      $$
      等式左边部分的意思是先用最佳价值函数 $V_{K+1}(s)$ 重构 $Q$ 函数，重构后每一列最大值对应的列名（动作 $a$）构成的动作序列，就是最佳策略。

    上述问题并不是强化学习的问题，而是动态规划的问题 

  - 价值迭代相当于使用终点状态的价值函数进行反向传播，每个状态只能影响到与其相关的状态，由最优性原理可知，状态 $s$ 到达最佳状态当且仅当其能到达的所有状态 $s$ 都达到了最佳状态，这个时候再去求每个状态下的最优动作才有意义，否则若是整个 `MDP` 还没收敛，那么这个时候的价值函数只是临时的、不完整的数据，只反映局部信息，不形成全局最优解，因此不能直接生成有效策略，即价值迭代的中间结果是无意义的。只有在收敛后，才能够通过策略提取来提取最佳策略。

策略迭代 v.s. 价值迭代

| 方面     | 策略迭代                                                     | 价值迭代                                                     |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 算法流程 | 策略评估计算当前策略的价值并迭代 $V_{\pi}$ 至收敛，策略改进使用 $Q$ 函数提取策略更新当前策略。重复*策略评估-策略改进* 直至收敛到 $\pi^*$ <br />同时计算 $V$ 与 $\pi$ ，保证策略改进的有效性 | 迭代贝尔曼最优方程，更新每个状态的价值函数，直到整个 `MDP` 达到 $V^*$ ，然后使用 $V^*$ 重构 $Q^*$ ，进而提取最佳策略<br />先一次性迭代 $V$ 至收敛，再提取 $\pi$ ，无需中间策略的稳定性 |
| 中间结果 | 每个中间结果都是优化后的策略，是有意义的，尽管非最优         | 中间结果只反映局部信息，是临时的、不完整的，不能形成策略，无意义 |
| 收敛条件 | $\pi$ 不再变化                                               | $V$ 不再变化 / $\|V_{k+1}-V_k\|<\epsilon$                    |
| 计算资源 | 需要迭代多次 $V$ ，计算量大                                  | 跳过显式策略优化，直接优化 $V$ ，一般更快                    |
| 适用场景 | 需要中间策略                                                 | 模型已知且只需最终优化策略                                   |
| 数学基础 | 策略改进定理                                                 | 贝尔曼最优方程                                               |

在无限精度下，策略迭代和价值迭代军收敛到最佳策略 $\pi^*$ ，但路径不同
